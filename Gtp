# Import the necessary library
from transformers import pipeline

# Initialize the LLM model pipeline
llm = pipeline("text-generation", model="gpt-3.5-turbo")

# Input text from the user
input_text = "Can you tell me how many mangoes are from Mexico?"

# Define a prompt that instructs the model to return a JSON object
prompt = f"""
Extract the information from the following sentence and return it in JSON format:
Input: "{input_text}"
Output: {{
    "quantity": "<number of mangoes>",
    "location": "<location>"
}}
"""

# Generate a response using the LLM
response = llm(prompt, max_length=100, return_full_text=False)[0]['generated_text']

# Convert the response string to a JSON object
response_json = json.loads(response.strip())

# Display the JSON response
response_json

import ollama

# Function to process human input and generate JSON output using the model, streaming the output
def process_input_stream(human_input):
    # Define the prompt with rules to generate the output in JSON format
    prompt = f"""
    You are a data extraction assistant. Extract the following information from the human text and output it in the following JSON format:

    Human text: "{human_input}"

    JSON format:
    {{
        "customer": "<customer_name>",
        "brand": "<brand_name>",
        "LOB": [
            {{
                "type": "<LOB_type>",
                "count": "<number_of_LOB>"
            }}
        ]
    }}

    Ensure 'brand', 'LOB type', and 'LOB count' are extracted correctly from the input.
    """

    # Stream the response from the model
    print("\nBot (JSON Output): ", end='', flush=True)
    for chunk in ollama.chat(model="llama-2", prompt=prompt, stream=True):
        # Print each chunk as it is generated
        print(chunk['text'], end='', flush=True)
    print()  # Add a newline after the output

# Main loop to interact with the user
def chatbot():
    print("Chatbot is running. Type 'exit' to stop the conversation.")
    
    while True:
        # Get input from the user
        human_input = input("\nYou: ")

        # Check if the user wants to exit
        if human_input.lower() == 'exit':
            print("Goodbye!")
            break

        # Process the input through the LLaMA model with streaming output
        process_input_stream(human_input)

# Run the chatbot
if __name__ == "__main__":
    chatbot()
